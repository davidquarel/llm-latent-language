Command: llama_experiment_template.py --intervention_func hook_reject_subspace --log_file hook_reject_alt_sweep --intervention_correct_latent_space False
{'dataset_filter_correct': True,
 'dataset_path': './data/synth_llama2',
 'debug': True,
 'dest_lang': 'zh',
 'end_layer_high': 32,
 'end_layer_low': 0,
 'hf_token': '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',
 'intervention_correct_latent_space': False,
 'intervention_func': 'hook_reject_subspace',
 'latent_lang': 'en',
 'log_file': 'hook_reject_alt_sweep',
 'model_name': 'meta-llama/Llama-2-7b-hf',
 'model_size': '7b',
 'multi_token_only': False,
 'num_multi_shot': 5,
 'out_dir': './visuals',
 'seed': 42,
 'single_token_only': False,
 'src_lang': 'fr',
 'start_layer_high': 32,
 'start_layer_low': 0,
 'steer_scale_coeff': 1.0,
 'token_add_leading_byte': False,
 'token_add_prefixes': False,
 'token_add_spaces': True,
 'use_tuned_lens': True}
 ==============
def hook_reject_subspace(
    resid: Float[Tensor, "batch seq dmodel"],
    hook: HookPoint,
    model,
    latent_ids : Int[Tensor, "num_latent_tokens"] = None,
    alt_latent_ids : Int[Tensor, "num_alt_latent_tokens"] = None,
    intervention_correct_latent_space : bool = True,
    **kwargs
) -> Float[Tensor, "batch seq dmodel"]:
    # modify attn_pattern (can be inplace)
    if intervention_correct_latent_space:
        subspace = model.unembed.W_U.T[latent_ids]
    else:
        subspace = model.unembed.W_U.T[alt_latent_ids]
        
    last_tblock = resid[:, -1]
    # subspace = W_U.T[latent_tok_ids]
    last_tblock = last_tblock - proj(last_tblock.float(), subspace.float())
    resid[:, -1] = last_tblock
    return resid

==============

Measuring 
lp_out/p_out : logprobs/probs of correct answer
lp_alt/p_alt logprobs/probs of alternate answer
lp_diff/p_ratio: logprob_diff/probs ration of alt-correct or alt/correct
