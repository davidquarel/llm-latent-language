Command: llama_experiment_template.py --use_tuned_lens False --intervention_correct_latent_space False --log_file out/reject_lang_sweep_alt/hook_reject_de_fr_zh --src_lang de --dest_lang zh --latent_lang fr
{'seed': 42,
 'src_lang': 'de',
 'dest_lang': 'zh',
 'latent_lang': 'fr',
 'model_size': '7b',
 'model_name': 'meta-llama/Llama-2-7b-hf',
 'single_token_only': False,
 'multi_token_only': False,
 'out_dir': './visuals',
 'hf_token': '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',
 'dataset_path': './data/synth_llama2',
 'debug': True,
 'num_multi_shot': 5,
 'token_add_spaces': True,
 'token_add_leading_byte': False,
 'token_add_prefixes': False,
 'dataset_filter_correct': True,
 'use_tuned_lens': False,
 'intervention_correct_latent_space': False,
 'steer_scale_coeff': 1.0,
 'start_layer_low': 0,
 'start_layer_high': 32,
 'end_layer_low': 0,
 'end_layer_high': 32,
 'intervention_func': 'hook_reject_subspace',
 'log_file': 'out/reject_lang_sweep_alt/hook_reject_de_fr_zh',
 'metric': 'p_alt',
 'metric_goal': 'max'}
==============
        temp_hook_fn = lambda resid, hook: self.apply(resid, hook, model, **kwargs)

==============

Measuring 
lp_out/p_out : logprobs/probs of correct answer
lp_alt/p_alt logprobs/probs of alternate answer
lp_diff/p_ratio: logprob_diff/probs ration of alt-correct or alt/correct

==============
size of dataset: 54size of correct dataset: 37